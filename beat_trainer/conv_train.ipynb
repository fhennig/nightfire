{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training to Detect Beat Position in Window\n",
    "\n",
    "The idea is to train a kernel to detect beats.  It is assumed that there is typically some sort of percussion on the beat, which the network can pick up on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import os.path\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion()\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load local code\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "import nf_train.data as nfd\n",
    "reload(nfd)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data contains 43 songs.\n",
      "0 songs failed to process.\n",
      "One second contains 100.0 samples.\n",
      "One sample has 30 features.\n"
     ]
    }
   ],
   "source": [
    "# initialize data source\n",
    "data_dir = nfd.DataDir('../nf_data/run2/')\n",
    "print(f\"Data contains {len(data_dir.processed)} songs.\")\n",
    "print(f\"{len(data_dir.failed)} songs failed to process.\")\n",
    "print(f\"One second contains {data_dir.params['rate']} samples.\")\n",
    "print(f\"One sample has {data_dir.params['n_filters']} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Training Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taps(beats):\n",
    "    \"\"\"Returns a tapped location based on a true beat location.\"\"\"\n",
    "    offsets = np.random.choice([-2, -1, 0, 1, 2],\n",
    "                              p=[0.05, 0.2, 0.5, 0.2, 0.05],\n",
    "                              size=len(beats))\n",
    "    taps = np.sum(np.vstack([beats, offsets]), axis=0)\n",
    "    return taps\n",
    "\n",
    "def get_target(tap, beat, r):\n",
    "    t = torch.zeros((r + 1 + r), dtype=torch.float)\n",
    "    i = beat - tap + r\n",
    "    t[i] = 1\n",
    "    return torch.tensor(i, dtype=torch.long)\n",
    "\n",
    "def get_sample(data, beat, tap, r, pre, post):\n",
    "    pre = pre + r\n",
    "    post = post + r\n",
    "    inp = data.get_hist_frame(tap, pre, post)\n",
    "    if inp is None:\n",
    "        return None\n",
    "    if torch.max(inp) < 1.2:\n",
    "        return None\n",
    "    target = get_target(tap, beat, r)\n",
    "    return {\n",
    "        'info': data.info,\n",
    "        'input': inp,\n",
    "        'target': target,\n",
    "    }\n",
    "\n",
    "def get_tr_va_data(data, r, pre, post, ratio):\n",
    "    \"\"\"data is a song object.\"\"\"\n",
    "    # get taps and beats\n",
    "    beat_locs = data.beat_indices\n",
    "    tap_locs = get_taps(beat_locs)\n",
    "    samples = [get_sample(data, beat, tap, r, pre, post)\n",
    "               for beat, tap in zip(beat_locs, tap_locs)]\n",
    "    samples = [s for s in samples if s is not None]\n",
    "    random.shuffle(samples)\n",
    "    print(f\"samples: {len(samples)}\")\n",
    "    split = round(len(samples) * ratio)\n",
    "    tr_samples, va_samples = samples[:split], samples[split:]\n",
    "    return tr_samples, va_samples\n",
    "\n",
    "def tr_va_from_files(data_dir, files, r, pre, post, ratio):\n",
    "    tr_samples, va_samples = [], []\n",
    "    for file in files:\n",
    "        data = data_dir.get_file(file)\n",
    "        t, v = get_tr_va_data(data, r, pre, post, ratio)\n",
    "        tr_samples.extend(t)\n",
    "        va_samples.extend(v)\n",
    "    return tr_samples, va_samples        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples: 592\n"
     ]
    }
   ],
   "source": [
    "r, pre, post = 2, 2, 2\n",
    "tr_ratio = 0.7\n",
    "d = data_dir.params['n_filters']\n",
    "\n",
    "files = [\n",
    "    '9397943_LSD_Original_Mix.pickle'\n",
    "]\n",
    "\n",
    "t, v = tr_va_from_files(data_dir, files, r, pre, post, tr_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv(nn.Module):\n",
    "    def __init__(self, r, pre, post, d, mean, scale):\n",
    "        super(SimpleConv, self).__init__()\n",
    "        self.t = pre + r + 1 + r + post\n",
    "        self.d = d\n",
    "        self.c = r + 1 + r\n",
    "        self.mean = mean.repeat(self.t, 1).unsqueeze(dim=0)\n",
    "        self.scale = scale.repeat(self.t, 1).unsqueeze(dim=0)\n",
    "        self.conv = nn.Conv2d(1, 1, (pre + 1 + post, d), bias=True)\n",
    "        self.pool = nn.Conv1d(1, 1, 5, bias=True, padding=2)\n",
    "        # conv layer with kernel (pre + 1 + post, d) -> (1, r + 1 + r)\n",
    "        # softmax\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        out = inp\n",
    "        out = (out - self.mean) * self.scale\n",
    "        out = self.conv(out)\n",
    "        out = out.squeeze(dim=3)\n",
    "        out = self.pool(out)\n",
    "        out = out.squeeze(dim=1)\n",
    "        #print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(samples):\n",
    "    return DataLoader(\n",
    "        samples,\n",
    "        shuffle=True,\n",
    "        batch_size=2,\n",
    "        collate_fn=lambda samples: {\n",
    "            'input': torch.unsqueeze(torch.stack([s['input'] for s in samples], axis=0), 1),\n",
    "            'target': torch.stack([s['target'] for s in samples], axis=0)\n",
    "        },\n",
    "    )\n",
    "\n",
    "def validate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    for sample in data_loader:\n",
    "        x = sample['input']\n",
    "        out = model(x)\n",
    "        beat = torch.argmax(out, axis=1)\n",
    "        #print(beat)\n",
    "        #print(sample['target'])\n",
    "        correct += torch.sum(beat == sample['target'])\n",
    "        count += len(sample['target'])\n",
    "    v = float(correct)/float(count)\n",
    "    return v\n",
    "\n",
    "def train(model, tr_set, va_set, epochs=10):\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    tr_loader = data_loader(tr_set)\n",
    "    va_loader = data_loader(va_set)\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        for sample in tr_loader:\n",
    "            x = sample['input']\n",
    "            y = sample['target']\n",
    "            out = model(x)\n",
    "            #print(f\"out: {out.shape} {y.shape}\")\n",
    "            loss = F.cross_entropy(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        tr_acc = validate_model(model, tr_loader)\n",
    "        va_acc = validate_model(model, va_loader)\n",
    "        print(f\"Acc tr: {tr_acc:.02f}; va: {va_acc:.02f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples: 593\n",
      "samples: 695\n",
      "samples: 961\n",
      "samples: 866\n",
      "samples: 677\n",
      "samples: 1062\n",
      "samples: 682\n",
      "samples: 694\n",
      "samples: 742\n",
      "samples: 657\n",
      "samples: 868\n",
      "samples: 620\n",
      "samples: 673\n",
      "samples: 443\n"
     ]
    }
   ],
   "source": [
    "r, pre, post = 2, 6, 12  # 10, 20\n",
    "tr_ratio = 0.7\n",
    "d = data_dir.params['n_filters']\n",
    "\n",
    "files = [\n",
    "    '9397943_LSD_Original_Mix.pickle',\n",
    "    '5799420_Rave_Original_Mix.pickle',\n",
    "    '8498819_Raita_Yksi_Original_Mix.pickle',\n",
    "    '9928622_Target_Line_featuring_Vril_Original_Mix.pickle',\n",
    "    '7811712_Edging_Forward_Original_Mix.pickle',\n",
    "    '9006939_Acid_Trip_Original_Mix.pickle',\n",
    "    '6450638_Iyewaye_Original_Mix.pickle',\n",
    "    '7600824_Tripical_Moon_Original_Mix.pickle',\n",
    "    '9928484_Orchid_Original_Mix.pickle',\n",
    "    '3887937_Pistolero_Astrix_Remix.pickle',\n",
    "    '10310700_Fucking_Dimensions_Original_Mix.pickle',\n",
    "    '8483511_Egypt_Stage_Original_Mix.pickle',\n",
    "    #'10 - Bratenschneider - Trigger.pickle',\n",
    "    '10924655_Take_Kurie__Original_Mix__Original_Mix.pickle',\n",
    "    #'06 - Nicorus - Canis Lupus.pickle',\n",
    "    #'8013109_Daydream_Original_Mix.pickle',\n",
    "    '8602606_Here_We_Go_Again_2016_Pro_Mix.pickle',\n",
    "]\n",
    "\n",
    "t, v = tr_va_from_files(data_dir, files, r, pre, post, tr_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_params(data):\n",
    "    scaler = StandardScaler()\n",
    "    for sample in tqdm(data):\n",
    "        for row in sample['input']:\n",
    "            row = np.array([row.detach().numpy()])\n",
    "            scaler.partial_fit(row)\n",
    "    return torch.tensor(scaler.mean_, dtype=torch.float), torch.tensor(scaler.scale_, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d6e4be4250842d1b278d8d68c384352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7162.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01afc9cf353a4eb984033cff0418af70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc tr: 0.81; va: 0.79\n",
      "Acc tr: 0.84; va: 0.83\n",
      "Acc tr: 0.85; va: 0.83\n",
      "Acc tr: 0.86; va: 0.83\n",
      "Acc tr: 0.86; va: 0.84\n",
      "Acc tr: 0.86; va: 0.84\n",
      "Acc tr: 0.87; va: 0.84\n",
      "Acc tr: 0.87; va: 0.85\n",
      "Acc tr: 0.87; va: 0.85\n",
      "Acc tr: 0.88; va: 0.85\n",
      "Acc tr: 0.88; va: 0.86\n",
      "Acc tr: 0.88; va: 0.86\n",
      "Acc tr: 0.88; va: 0.85\n",
      "Acc tr: 0.88; va: 0.86\n",
      "Acc tr: 0.89; va: 0.86\n",
      "Acc tr: 0.88; va: 0.86\n",
      "Acc tr: 0.89; va: 0.86\n",
      "Acc tr: 0.89; va: 0.86\n",
      "Acc tr: 0.89; va: 0.86\n",
      "Acc tr: 0.89; va: 0.86\n",
      "Acc tr: 0.88; va: 0.86\n",
      "Acc tr: 0.89; va: 0.86\n",
      "Acc tr: 0.89; va: 0.86\n",
      "Acc tr: 0.89; va: 0.86\n",
      "Acc tr: 0.89; va: 0.87\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mean, scale = norm_params(t)\n",
    "model = SimpleConv(r, pre, post, d, mean, scale)\n",
    "train(model, t, v, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO Analysis\n",
    "\n",
    "Make a confusion matrix.  How far off is the prediction?\n",
    "\n",
    "Also, which song causes the most confusion?  I suspect that certain songs are much more difficult than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
